{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Mechanism\n",
    "This notebook contains the code used to reproduce the mechanism of (Yue et al., 2021). This code was used in Section 7 of the paper to compare it with our fixed mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Add the main directory to sys.path to be able to import config\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import ROOT_DIR\n",
    "from utils.tools import compute_distances, rank_neighbors\n",
    "\n",
    "# PARAMS\n",
    "distance_metric = \"euclidean\" # Metric to use for the distances\n",
    "distances_dtype = np.float32 # Precision of the distances \n",
    "# !!! float32 (at least) needed to avoid overflow when computing probabilities\n",
    "\n",
    "glove_variant = \"6B\" #\"Twitter\" or \"6B\"\n",
    "hidden_size = 300\n",
    "glove_data_folderpath = ROOT_DIR\n",
    "\n",
    "# END PARAMS\n",
    "if glove_variant == \"6B\":\n",
    "    glove_dimension_to_filename = {\n",
    "        50: \"glove.6B.50d.pkl\", # 400000 words\n",
    "        100:\"glove.6B.100d.pkl\", # 400000 words\n",
    "        200: \"glove.6B.200d.pkl\", # 400000 words\n",
    "        300:\"glove.6B.300d.pkl\" # 400000 words\n",
    "    }\n",
    "elif glove_variant == \"Twitter\":\n",
    "    glove_dimension_to_filename = {\n",
    "        25: \"glove.twitter.27B.25d.pkl\", #1,193,513 words\n",
    "        50: \"glove.twitter.27B.50d.pkl\", #1,193,513 words\n",
    "        100: \"glove.twitter.27B.100d.pkl\", #1,193,513 words\n",
    "        200: \"glove.twitter.27B.200d.pkl\", #1,193,513 words\n",
    "    }\n",
    "fit_dtype = np.uint32 # Integer size sufficient to encode the number of words in the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GloVe in a specific dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "    glove = pickle.load(f)\n",
    "\n",
    "vocab_embs = np.array(list(glove.values()))\n",
    "vocab_size = vocab_embs.shape[0]\n",
    "del glove # Save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *number_of_words* random words and compute their distance with all the words in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 5000\n",
    "words_ids = np.random.randint(0, vocab_size, size=number_of_words)\n",
    "words_embs = vocab_embs[words_ids]\n",
    "\n",
    "# Get the distances between the selected words and the entire vocabulary\n",
    "distances = compute_distances(words_embs, vocab_embs, distance_metric, dtype=distances_dtype)\n",
    "\n",
    "# Also rank the entire vocabulary\n",
    "words_neighbors_ranked = rank_neighbors(words_embs, vocab_embs, distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the SANTEXT mechanism detailed in Algorithm 1 of (Yue et al., 2021). For each word $\\mathbf{x}$ selected above, compute the probability that they are substituted by each word $\\mathbf{y}$ in the vocabulary:\n",
    "$$\\frac{\\exp(- \\frac{1}{2} \\epsilon d_\\text{euc}(\\mathbf{x}, \\mathbf{y}))}{\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\exp(- \\frac{1}{2} \\epsilon d_\\text{euc}(\\mathbf{x}, \\mathbf{y}))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.arange(0.5, 6.1, 0.1) # Parameter depending on the GloVe version selected\n",
    "neighbor_counted_occurences = {}\n",
    "\n",
    "# Process epsilons in parallel\n",
    "n_workers = 2\n",
    "\n",
    "def process_epsilon(epsilon):\n",
    "    # Compute the probabilities\n",
    "    probabilities = softmax(-1/2*epsilon*distances, axis=-1)\n",
    "\n",
    "    # Sample one replacement for each word\n",
    "    noisy_word_ids = [np.random.choice(vocab_size, p=probabilities[i]) for i in range(number_of_words)]\n",
    "\n",
    "    # for all words_ids, get the rank k of noisy_word_ids[i] and increase a counter at index k\n",
    "    noisy_word_ids_ranks = words_neighbors_ranked[np.arange(number_of_words), noisy_word_ids] # This line, for all the elements i in the first dimension of words_neighbors_ranked, gets the particular value pointed by the index which is stored at noisy_word_ids[i]\n",
    "\n",
    "    noisy_word_ids_ranks_counted = Counter(noisy_word_ids_ranks)\n",
    "    return [noisy_word_ids_ranks_counted[k] for k in range(vocab_size)]\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    results = list(executor.map(process_epsilon, epsilons))\n",
    "\n",
    "neighbor_counted_occurences = dict(zip(epsilons, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *neighbor_counted_occurences*, which is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with a list, where list[i] contains the number of times the i-th neighbor was chosen as the replacement of a word. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dx-privacy-old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
