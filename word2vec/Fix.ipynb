{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Neighbors $d_\\mathcal{X}$-privacy frequencies with fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the main directory to sys.path to be able to import config\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import ROOT_DIR\n",
    "from utils.dx import sample_noise_vectors, noisy_embeddings_to_ids_with_post_processing_fix\n",
    "from utils.tools import rank_neighbors\n",
    "\n",
    "# PARAMS\n",
    "number_of_words = 5000\n",
    "# END PARAMS\n",
    "\n",
    "distance_metric = \"euclidean\"\n",
    "distances_dtype = np.float16  # Precision of the distances\n",
    "\n",
    "word2vec_data_folderpath = ROOT_DIR\n",
    "fit_dtype = (\n",
    "    np.uint32\n",
    ")  # Integer size sufficient to encode the number of words in the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils/dx.py contains noisy_embeddings_to_ids_with_post_processing_fix. This function includes the post-processing step after we have found the nearest word $\\mathbf{x}^*$ to the noisy embedding $\\mathbf{w}^*$. We sort the nearest neighbors of $\\mathbf{x}^*$ and output a neighbor proportional to $\\exp(- d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))$. More specifically any word $\\mathbf{x} \\in \\mathcal{D}$ is output with probability:\n",
    "$$\\frac{\\exp(- c \\epsilon d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))}{\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\exp(- c \\epsilon d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))}, \n",
    "$$\n",
    "where $c$ is a constant to control how many neighbors are likely to be selected. A higher value such as $c > 1$ means that the mechanism will output the first few neighbors with high probability, and a lower value such as $c = 0.01$ means that more neighbors will likely to be output, of course, with probability exponentially decreasing as we move away from the original word. This is the same as the temperature variable in the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    join(word2vec_data_folderpath, \"GoogleNews-vectors-negative300.pkl\"), \"rb\"\n",
    ") as f:\n",
    "    word2vec = pickle.load(f)\n",
    "\n",
    "vocab_embs = np.array(list(word2vec.values()))\n",
    "vocab_size = vocab_embs.shape[0]\n",
    "hidden_size = vocab_embs.shape[1]\n",
    "del word2vec  # Save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *number_of_words* random words and rank their neighbors according to their distance with the word in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ids = np.random.choice(vocab_size, size=number_of_words, replace=False)\n",
    "words_embs = vocab_embs[words_ids]\n",
    "\n",
    "del words_ids  # Save RAM\n",
    "words_neighbors_ranked = rank_neighbors(words_embs, vocab_embs, distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add noise to the embeddings of the words following the $d_x$-privacy mechanism. Apply the post-processing described in the paper and count which neighbor was chosen, represented by its rank in the neighbor list of the initial word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [i for i in range(1, 302, 5)]\n",
    "dx_constant = 0.007\n",
    "neighbor_counted_occurences = {}\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    embeddings = np.copy(words_embs)\n",
    "    noise = sample_noise_vectors(\n",
    "        dimension=hidden_size, shape1=1, shape2=number_of_words, epsilon=epsilon\n",
    "    )[0]\n",
    "    # Adding noise to embeddings\n",
    "    noisy_embeddings = embeddings + noise\n",
    "\n",
    "    del noise  # Save RAM\n",
    "    del embeddings  # Save RAM\n",
    "\n",
    "    # We first find the nearest neighbors of each of the noisy embeddings, called the \"pivots\" here\n",
    "    # Then, we apply the post-processing fix proposed in the paper, by sampling a neighbor\n",
    "    # of each pivot according to the formula above. Finally, w\n",
    "    noisy_words_ids = noisy_embeddings_to_ids_with_post_processing_fix(\n",
    "        noisy_embeddings, vocab_embs, dx_constant, epsilon, distance_metric\n",
    "    )\n",
    "\n",
    "    # We count the number of times the k-th neighbor has been chosen and store it in neighbor_counted_occurences.\n",
    "\n",
    "    neighbor_counted_occurences[epsilon] = {}\n",
    "    # for all words_ids, get the rank k of noisy_word_ids[i] and increase a counter at index k\n",
    "    noisy_word_ids_ranks = words_neighbors_ranked[\n",
    "        np.arange(number_of_words), noisy_words_ids\n",
    "    ]  # This line, for all the elements i in the first dimension of words_neighbors_ranked, gets the particular value pointed by the index which is stored at noisy_word_ids[i]\n",
    "    noisy_word_ids_ranks_counted = Counter(noisy_word_ids_ranks)\n",
    "    neighbor_counted_occurences[epsilon][dx_constant] = [\n",
    "        noisy_word_ids_ranks_counted[k] for k in range(vocab_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *neighbor_counted_occurences*, which is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with another dictionary, where the keys are floats representing the value of the constant $c$ in the post-processing fix. This sub-dictionnary associates each $c$ with a list, where list[i] contains the number of times the i-th neighbor was chosen as the replacement of a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "close_neighbors_max_rank = 100 # The maximum rank (including) of what is considered a \"close\" neighbor \n",
    "\n",
    "initial_word_frequency = np.array([neighbor_counted_occurences[i][dx_constant][0] for i in epsilons[hidden_size]])\n",
    "close_neighbors_frequency = np.array([sum(neighbor_counted_occurences[i][dx_constant][1:close_neighbors_max_rank+1]) for i in epsilons[hidden_size]])\n",
    "distant_neighbors_frequency = np.array([sum(neighbor_counted_occurences[i][dx_constant][close_neighbors_max_rank+1:]) for i in epsilons[hidden_size]])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(epsilons[hidden_size], initial_word_frequency/number_of_words, label=\"Original value\", linewidth=1.5, markersize=5)\n",
    "ax.plot(epsilons[hidden_size], distant_neighbors_frequency/number_of_words, label=\"Distant neighbors\", linewidth=1.5,linestyle='dashed')\n",
    "ax.plot(epsilons[hidden_size], close_neighbors_frequency/number_of_words, label=\"Close neighbors\", linewidth=1.5, marker=\".\")\n",
    "\n",
    "ax.set_xlabel(\"Ïµ\")\n",
    "ax.set_ylabel(\"Proportion of the output\")\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "fig.text(0.17, 0.80, f\"c={dx_constant:.3f}\", fontsize=16)\n",
    "#ax.set_xlim(0,50)\n",
    "ax.set_ylim(-0.05,1.05)\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dx-privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
