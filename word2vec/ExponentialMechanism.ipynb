{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Mechanism\n",
    "This notebook contains the code used to reproduce the mechanism of (Yue et al., 2021). This code was used in Section 7 of the paper to compare it with our fixed mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the main directory to sys.path to be able to import config\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import ROOT_DIR\n",
    "from utils.tools import rank_neighbors, compute_distances\n",
    "\n",
    "# PARAMS\n",
    "number_of_words = 5000\n",
    "# END PARAMS\n",
    "\n",
    "distance_metric = \"euclidean\"\n",
    "distances_dtype = np.float32  # Precision of the distances\n",
    "# !!! float32 (at least) needed to avoid overflow when computing probabilities\n",
    "\n",
    "word2vec_data_folderpath = ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    join(word2vec_data_folderpath, \"GoogleNews-vectors-negative300.pkl\"), \"rb\"\n",
    ") as f:\n",
    "    word2vec = pickle.load(f)\n",
    "\n",
    "vocab_embs = np.array(list(word2vec.values()))\n",
    "vocab_size = vocab_embs.shape[0]\n",
    "hidden_size = vocab_embs.shape[1]\n",
    "del word2vec  # Save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *number_of_words* random words and rank their neighbors according to their distance with the word in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ids = np.random.choice(vocab_size, size=number_of_words, replace=False)\n",
    "words_embs = vocab_embs[words_ids]\n",
    "\n",
    "# Get the distances between the selected words and the entire vocabulary\n",
    "distances = compute_distances(\n",
    "    words_embs, vocab_embs, distance_metric, dtype=distances_dtype\n",
    ")\n",
    "\n",
    "# Also rank the entire vocabulary\n",
    "words_neighbors_ranked = rank_neighbors(words_embs, vocab_embs, distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the SANTEXT mechanism detailed in Algorithm 1 of (Yue et al., 2021). For each word $\\mathbf{x}$ selected above, compute the probability that they are substituted by each word $\\mathbf{y}$ in the vocabulary:\n",
    "$$\\frac{\\exp(- \\frac{1}{2} \\epsilon d_\\text{euc}(\\mathbf{x}, \\mathbf{y}))}{\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\exp(- \\frac{1}{2} \\epsilon d_\\text{euc}(\\mathbf{x}, \\mathbf{y}))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [i for i in range(1, 41)]\n",
    "neighbor_counted_occurences = {}\n",
    "\n",
    "# Process epsilons in parallel\n",
    "n_workers = 1\n",
    "\n",
    "\n",
    "def process_epsilon(epsilon):\n",
    "    # Compute the probabilities\n",
    "    probabilities = softmax(-1 / 2 * epsilon * distances, axis=-1)\n",
    "\n",
    "    # Sample one replacement for each word\n",
    "    noisy_word_ids = [\n",
    "        np.random.choice(vocab_size, p=probabilities[i]) for i in range(number_of_words)\n",
    "    ]\n",
    "\n",
    "    # for all words_ids, get the rank k of noisy_word_ids[i] and increase a counter at index k\n",
    "    noisy_word_ids_ranks = words_neighbors_ranked[\n",
    "        np.arange(number_of_words), noisy_word_ids\n",
    "    ]  # This line, for all the elements i in the first dimension of words_neighbors_ranked, gets the particular value pointed by the index which is stored at noisy_word_ids[i]\n",
    "\n",
    "    noisy_word_ids_ranks_counted = Counter(noisy_word_ids_ranks)\n",
    "    return [noisy_word_ids_ranks_counted[k] for k in range(vocab_size)]\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    results = list(executor.map(process_epsilon, epsilons))\n",
    "\n",
    "neighbor_counted_occurences = dict(zip(epsilons, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *neighbor_counted_occurences*, which is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with a list, where list[i] contains the number of times the i-th neighbor was chosen as the replacement of a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "close_neighbors_max_rank = 100 # The maximum rank (including) of what is considered a \"close\" neighbor \n",
    "\n",
    "initial_word_frequency = np.array([neighbor_counted_occurences[i][0] for i in epsilons])\n",
    "close_neighbors_frequency = np.array([sum(neighbor_counted_occurences[i][1:close_neighbors_max_rank+1]) for i in epsilons])\n",
    "distant_neighbors_frequency = np.array([sum(neighbor_counted_occurences[i][close_neighbors_max_rank+1:]) for i in epsilons])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.5, 2.62))\n",
    "\n",
    "ax.plot(epsilons, initial_word_frequency/number_of_words, label=\"Original value\", linewidth=1.5, markersize=5)\n",
    "ax.plot(epsilons, distant_neighbors_frequency/number_of_words, label=\"Distant neighbors\", linewidth=1.5,linestyle='dashed')\n",
    "ax.plot(epsilons, close_neighbors_frequency/number_of_words, label=\"Close neighbors\", linewidth=1.5, marker=\".\", markevery=10)\n",
    "\n",
    "ax.set_xlabel(\"$\\epsilon$\")\n",
    "ax.set_ylabel(\"Proportion of the output\")\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "ax.set_xlim(0)\n",
    "ax.legend(framealpha=0.4, loc=\"right\")\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dx-privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
